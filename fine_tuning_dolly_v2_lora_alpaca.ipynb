{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWr34kdl3AP"
      },
      "source": [
        "# Fine-tuning Dolly 2.0 with LoRa\n",
        "\n",
        "*   Dolly-v2-3b - https://huggingface.co/databricks/dolly-v2-3b\n",
        "*   LoRa paper - https://arxiv.org/abs/2106.09685\n",
        "*   Alpaca Cleaned Dataset - https://github.com/gururise/AlpacaDataCleaned\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCrBi94qAfFY",
        "outputId": "630fe6f0-e4a8-4798-9a55-bfae6c1e84c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AlpacaDataCleaned'...\n",
            "remote: Enumerating objects: 744, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 744 (delta 62), reused 91 (delta 51), pack-reused 623\u001b[K\n",
            "Receiving objects: 100% (744/744), 76.51 MiB | 23.74 MiB/s, done.\n",
            "Resolving deltas: 100% (409/409), done.\n",
            "Updating files: 100% (69/69), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/gururise/AlpacaDataCleaned.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N29PQadBd18",
        "outputId": "a038798b-c01a-4cd7-868c-26a6deffba2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpaca_data_cleaned_archive.json  \u001b[0m\u001b[01;34meval\u001b[0m/                    README.md\n",
            "alpaca_data_cleaned.json          generate_instruction.py  requirements.txt\n",
            "alpaca_data.json                  \u001b[01;34mgui\u001b[0m/                     schema.json\n",
            "alpacaModifier.py                 LICENSE                  seed_tasks.jsonl\n",
            "\u001b[01;34massets\u001b[0m/                           modifierGui.py           \u001b[01;34mtools\u001b[0m/\n",
            "DATA_LICENSE                      prompt.txt               utils.py\n",
            "\u001b[01;34mdataset_extensions\u001b[0m/               pyproject.toml\n"
          ]
        }
      ],
      "source": [
        "ls AlpacaDataCleaned/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMAo3SKL5Nmk",
        "outputId": "97f3b3a0-e26e-4bb8-bf1b-f19f4e53e9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install accelerate>=0.12.0 transformers[torch]==4.25.1\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Instruct Pipeline\n",
        "import logging\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from transformers import Pipeline, PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "INSTRUCTION_KEY = \"### Instruction:\"\n",
        "RESPONSE_KEY = \"### Response:\"\n",
        "END_KEY = \"### End\"\n",
        "INTRO_BLURB = (\n",
        "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        ")\n",
        "\n",
        "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
        "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
        "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
        "{instruction_key}\n",
        "{instruction}\n",
        "{response_key}\n",
        "\"\"\".format(\n",
        "    intro=INTRO_BLURB,\n",
        "    instruction_key=INSTRUCTION_KEY,\n",
        "    instruction=\"{instruction}\",\n",
        "    response_key=RESPONSE_KEY,\n",
        ")\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
        "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
        "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "\n",
        "class InstructionTextGenerationPipeline(Pipeline):\n",
        "    def __init__(\n",
        "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)\n",
        "\n",
        "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
        "        preprocess_params = {}\n",
        "\n",
        "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
        "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
        "        tokenizer_response_key = next(\n",
        "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
        "        )\n",
        "\n",
        "        response_key_token_id = None\n",
        "        end_key_token_id = None\n",
        "        if tokenizer_response_key:\n",
        "            try:\n",
        "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
        "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
        "\n",
        "                # Ensure generation stops once it generates \"### End\"\n",
        "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        forward_params = generate_kwargs\n",
        "        postprocess_params = {\n",
        "            \"response_key_token_id\": response_key_token_id,\n",
        "            \"end_key_token_id\": end_key_token_id,\n",
        "            \"return_instruction_text\": return_instruction_text,\n",
        "        }\n",
        "\n",
        "        return preprocess_params, forward_params, postprocess_params\n",
        "\n",
        "    def preprocess(self, instruction_text, **generate_kwargs):\n",
        "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
        "        inputs = self.tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs[\"prompt_text\"] = prompt_text\n",
        "        inputs[\"instruction_text\"] = instruction_text\n",
        "        return inputs\n",
        "\n",
        "    def _forward(self, model_inputs, **generate_kwargs):\n",
        "        input_ids = model_inputs[\"input_ids\"]\n",
        "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
        "        generated_sequence = self.model.generate(\n",
        "            input_ids=input_ids.to(self.model.device),\n",
        "            attention_mask=attention_mask,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            **generate_kwargs,\n",
        "        )[0].cpu()\n",
        "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
        "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
        "\n",
        "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_instruction_text):\n",
        "        sequence = model_outputs[\"generated_sequence\"]\n",
        "        instruction_text = model_outputs[\"instruction_text\"]\n",
        "\n",
        "        # The response will be set to this variable if we can identify it.\n",
        "        decoded = None\n",
        "\n",
        "        # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
        "        if response_key_token_id and end_key_token_id:\n",
        "            # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
        "            # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
        "            response_pos = None\n",
        "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
        "            if len(response_positions) == 0:\n",
        "                logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
        "            else:\n",
        "                response_pos = response_positions[0]\n",
        "\n",
        "            if response_pos:\n",
        "                # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
        "                # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
        "                # this token, as the response could be truncated.  If we don't find it then just return everything\n",
        "                # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
        "                end_pos = None\n",
        "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
        "                if len(end_positions) > 0:\n",
        "                    end_pos = end_positions[0]\n",
        "\n",
        "                decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
        "        else:\n",
        "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
        "\n",
        "            fully_decoded = self.tokenizer.decode(sequence)\n",
        "\n",
        "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
        "            # end.\n",
        "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
        "\n",
        "            if m:\n",
        "                decoded = m.group(1).strip()\n",
        "            else:\n",
        "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
        "                # return everything after \"### Response:\".\n",
        "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
        "                if m:\n",
        "                    decoded = m.group(1).strip()\n",
        "                else:\n",
        "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
        "\n",
        "        if return_instruction_text:\n",
        "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "SShylzv2uMoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-3b\", \n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=torch.bfloat16)\n",
        "\n",
        "# generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "NiZPnWpWu2hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKPVZV6IBHhi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"json\", \n",
        "                    data_files=\"./AlpacaDataCleaned/alpaca_data.json\")\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    # taken from https://github.com/tloen/alpaca-lora\n",
        "    if data_point[\"instruction\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "data = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRHBoBIIBtqB"
      },
      "source": [
        "## Finetuning Dolly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSfkRYg9BZkU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, GPTJForCausalLM\n",
        "\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfRmmxGBB0-h"
      },
      "outputs": [],
      "source": [
        "# Settings for A100 - For 3090 \n",
        "MICRO_BATCH_SIZE = 4  # change to 4 for 3090\n",
        "BATCH_SIZE = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 2  # paper uses 3\n",
        "LEARNING_RATE = 2e-5  \n",
        "CUTOFF_LEN = 256  \n",
        "LORA_R = 4\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZSJQ3PtDQVU"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_int8_training(model, \n",
        "                                        use_gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS4jUAJpDf-u"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "data = load_dataset(\"json\", data_files=\"./AlpacaDataCleaned/alpaca_data_cleaned.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jWp32UaDy1E"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle().map(\n",
        "    lambda data_point: tokenizer(\n",
        "        generate_prompt(data_point),\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8iz9ku1FIIW"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlw7IxfUED0a"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"lora-dolly\",\n",
        "        save_total_limit=3,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "model.save_pretrained(\"alpaca-lora-dolly-2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ujd5GCMRC0xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Look up the boiling point of water.\")"
      ],
      "metadata": {
        "id": "CdFkhZe-clpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Find the capital of Spain.\")"
      ],
      "metadata": {
        "id": "ba2cPH5YdR5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Translate the following phrase into French: I love my dog\")"
      ],
      "metadata": {
        "id": "l8bqQ88Vdpku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Given a set of numbers, find the maximum value: Set: {10, 3, 25, 62, 16}\")"
      ],
      "metadata": {
        "id": "43kOf72Zd5d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1yDXCFNmM5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}